{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/diegohermosillo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/diegohermosillo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "import glob\n",
    "import nltk\n",
    "import re\n",
    "import math\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = list(punctuation)\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing(text):\n",
    "    # quitar espacios extra en los lados y pasar a minúsculas\n",
    "    new_text = text.strip().lower()\n",
    "    # quitar doble espacio\n",
    "    new_text = new_text.replace('  ', '')\n",
    "    new_text = new_text.replace(\"'\", '')\n",
    "    new_text = new_text.replace(\"-\", '')\n",
    "    new_text = re.sub(\"[^a-zA-Z\\s]+\", '', new_text)\n",
    "    # separar texto en tokens\n",
    "    tokens = word_tokenize(new_text)\n",
    "    # guardar tokens que no estén en las stopwords ni sean signos de puntuación\n",
    "    cleaned_tokens = [token for token in tokens if token not in stop_words \n",
    "                      and token not in punctuation and len(token) > 1]\n",
    "    \n",
    "    # stemmer\n",
    "    ps = PorterStemmer()\n",
    "    stem_tokens = [ps.stem(token) for token in cleaned_tokens]\n",
    "\n",
    "    return stem_tokens\n",
    "\n",
    "def read_processed_file(file):\n",
    "    # Función que lee un archivo procesado\n",
    "    f = open(file, 'r')\n",
    "    text = f.read()\n",
    "    f.close()\n",
    "    new_text = text.strip()\n",
    "    print(new_text.split('\\n'))\n",
    "\n",
    "def create_processed_file(tokens):\n",
    "    # Función que guarda el texto procesado en otro archivo\n",
    "    with open('01', 'w') as file:\n",
    "        for t in tokens:\n",
    "            file.write(f'{t}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not os.path.exists('procesados'):\n",
    "#     os.mkdir('procesados')\n",
    "\n",
    "# definir directorio de archivos aún sin procesar\n",
    "directory = 'sinprocesar'\n",
    "\n",
    "# numero de archivos y lista de archivos ordenados\n",
    "ordered_files = sorted(glob.glob(f'{directory}/*'))\n",
    "num = len(ordered_files)\n",
    "\n",
    "\n",
    "tf = {}\n",
    "n = {}\n",
    "# iterar sobre los archivos en ese directorio\n",
    "for id, filename in enumerate(ordered_files):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    tokens = processing(text)\n",
    "    \n",
    "    for t in set(tokens):\n",
    "        # guarda como llave el termino y como valor una lista de tamaño N y después se modifica el tf\n",
    "        if t not in tf:\n",
    "            tf[t] = {}\n",
    "        tf[t][id] = 1 + math.log2(tokens.count(t))\n",
    "        # guarda el conteo de las palabras\n",
    "        n[t] = n.get(t,0) + tokens.count(t)\n",
    "\n",
    "#del punctuation, ordered_files, directory, tokens\n",
    "# A n sacar el idf = log(N/ni) donde\n",
    "#           N : Número total de documentos\n",
    "#           ni : número de documentos donde aparece (o sea el conteo)\n",
    "\n",
    "idf = {k:math.log2(num/v) for (k,v) in n.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = {}\n",
    "for k,v in idf.items():\n",
    "    diccionario = tf.get(k)\n",
    "    tf_idf[k] = {k1: v1 * v for k1, v1 in diccionario.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "punctuation = list(punctuation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definir directorio de archivos aún sin procesar\n",
    "directory = 'sinprocesar'\n",
    "\n",
    "# numero de archivos y lista de archivos ordenados\n",
    "ordered_files = sorted(glob.glob(f'{directory}/*'))\n",
    "num = len(ordered_files)\n",
    "\n",
    "longitud = []\n",
    "\n",
    "for id, filename in enumerate(ordered_files):\n",
    "    valor = 0\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    tokens = processing(text)\n",
    "\n",
    "    for t in set(tokens):\n",
    "        valor += tf_idf[t][id]\n",
    "        \n",
    "    longitud.append(math.sqrt(math.pow(valor,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(data, name):\n",
    "    with open(f\"{name}.pkl\", 'wb') as fp:\n",
    "        pickle.dump(data, fp)\n",
    "\n",
    "def read_file(name):\n",
    "    with open(f\"{name}.pkl\", 'rb') as fp:\n",
    "        return pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dictionary to tf_idf, idf and longitud file\n",
    "save_file(tf_idf, \"tf_idf\")\n",
    "save_file(idf, \"idf\")\n",
    "save_file(longitud, \"longitud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 's'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[216], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#Sección 3: Búsqueda de 1 palabra (dividiendo entre la longitud)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m consulta \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mPlease make the query (1 word only): \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m num_results\u001b[39m=\u001b[39m\u001b[39mint\u001b[39;49m(\u001b[39minput\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mHow many results would you like to see?\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m      4\u001b[0m consulta_procesada\u001b[39m=\u001b[39m\u001b[39mstr\u001b[39m(processing(consulta)) \u001b[39m#El mismo pre procesamiento que se le hizo a los documentos se le aplica a la consulta\u001b[39;00m\n\u001b[1;32m      5\u001b[0m consulta_procesada \u001b[39m=\u001b[39m consulta_procesada\u001b[39m.\u001b[39mstrip(\u001b[39m\"\u001b[39m\u001b[39m[]\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 's'"
     ]
    }
   ],
   "source": [
    "#Sección 3: Búsqueda de 1 palabra (dividiendo entre la longitud)\n",
    "consulta = input(\"Please make the query (1 word only): \")\n",
    "num_results=int(input(\"How many results would you like to see?\"))\n",
    "consulta_procesada=str(processing(consulta)) #El mismo pre procesamiento que se le hizo a los documentos se le aplica a la consulta\n",
    "consulta_procesada = consulta_procesada.strip(\"[]'\")\n",
    "try:\n",
    "    resultados_ranking = {i: tf_idf[consulta_procesada][i] / longitud[i] for i in tf_idf[consulta_procesada].keys()} #Las llaves son los documentos\n",
    "    ranking_ordenado=sorted(resultados_ranking.items(), key=lambda item: item[1],reverse=True)[:num_results] #Aquí ya los ordena\n",
    "    for key,value in ranking_ordenado:\n",
    "        print(f\"Document number {key} with a weight of {value}\")\n",
    "except KeyError:\n",
    "    print(\"No results\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Búsquedas con más de 1 palabra con formato AND\n",
    "#Sirve con queries como \"how many galaxies\"\n",
    "consulta_and = input(\"Please make the query. This is an AND query.\")\n",
    "num_results_and=int(input(\"How many results would you like to see?\"))\n",
    "consulta_procesada_and = str(processing(consulta_and))  # Apply the same preprocessing as done for the documents to the query\n",
    "terminos = consulta_procesada_and.strip(\"[]'\").replace(\"'\", \"\").split(\", \")\n",
    "\n",
    "# Encontrar documentos que tengan los términos (ya procesados) de la búsqueda\n",
    "try:\n",
    "    docs_comunes = set.intersection(*(set(tf[i].keys()) for i in terminos))\n",
    "    resultados_and={}\n",
    "    for index, doc_number in enumerate(docs_comunes):\n",
    "        total_tfidf = sum(tf_idf[termino][doc_number] for termino in terminos) #Se suman los tf_idf de cada termino a nivel documento\n",
    "        resultados_and[doc_number]=total_tfidf\n",
    "\n",
    "    resultados_and_ordenados=sorted(resultados_and.items(),key=lambda item: item[1],reverse=True)[:num_results_and]\n",
    "    for key, value in enumerate(resultados_and_ordenados):\n",
    "        print(f\"Document number {key} with a weight of {value}\")\n",
    "\n",
    "except KeyError:\n",
    "    print(\"No results\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('sinprocesar/000309446800033', 12), ('sinprocesar/000286385600046', 10)]\n"
     ]
    }
   ],
   "source": [
    "#Consulta con comillas\n",
    "#Hacer la búsqueda con black hole\n",
    "consulta_comillas=input(\"Please make your query (between quotes \" \")\").strip('\"').lower()\n",
    "num_results_comillas=int(input(\"How many results would you like to see?\"))\n",
    "resultados_comillas={}\n",
    "try:\n",
    "    for id, filename in enumerate(ordered_files):\n",
    "        file = open(filename, 'r')\n",
    "        text = file.read().lower()\n",
    "        file.close()\n",
    "        if consulta_comillas in text:\n",
    "            resultados_comillas[filename]=text.count(consulta_comillas) #Elegimos hacer el ranking únicamente contando cuántas veces aparece la palabra\n",
    "\n",
    "    resultados_comillas_ordenados=sorted(resultados_comillas.items(),key=lambda item: item[1],reverse=True)[:num_results_comillas]\n",
    "    print(resultados_comillas_ordenados)\n",
    "except KeyError:\n",
    "    print(\"No results\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
